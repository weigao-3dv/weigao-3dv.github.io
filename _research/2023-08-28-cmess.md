---
title: "Cross-modal Learning for Event-based Semantic Segmentation via Attention Soft Alignment"
excerpt: "By demonstrating robustness in scenarios characterized by high-speed motion and extreme lighting changes, event cameras hold great potential for enhancing the perception reliability of autonomous driving systems. Since its novelty and data sparsity, the progress of event-based algorithms is hindered by the scarcity of high-quality labeled datasets. [**Read More**] <br/><img src='/images/research/CMESS-framework.png' width='500'>"
collection: research
---



## Abstract

By demonstrating robustness in scenarios characterized by high-speed motion and extreme lighting changes, event cameras hold great potential for enhancing the perception reliability of autonomous driving systems. Since its novelty and data sparsity, the progress of event-based algorithms is hindered by the scarcity of high-quality labeled datasets. In this work, we proposed CMESS (Cross-Modal learning for Event-based Semantic Segmentation), which eliminates the need for event labels by transferring the model from labeled image datasets (source domain) to unlabeled event datasets (target domain) via unsupervised domain adaptation (UDA). Compared to existing UDA methods that require hard alignment of visually consistent embeddings, our approach achieves soft alignment via cross-attention and then augments it with knowledge distillation to convey fine-grained source knowledge to the target domain. Additionally, we also proposed an event-driven bidirectional self-labeling method to generate weakly supervised signals for event-only datasets. These facilitate cross-modal learning without requiring per-pixel paired frames or online reconstruction. Experimental results show that our method outperforms existing state-of-the-art methods in both UDA and supervised settings on common evaluation benchmarks, making it a universal framework for further unlabeled event-related visual tasks.

## Framework

The overview of CMESS is presented in above figure, which comprises three stages: Source-Path Initialization (SPI), Target-Path Labeling (TPL), and Cross-modal Learning (CML). During training, our method undergoes three-stage to transfer a task from the image to the event domain. Stage 1, SPI, initializes source and synthetic event models from the source dataset, forming the basis for subsequent stages. In stage 2, TPL, independent inference on the target dataset and merging their predictions to generate event pseudo-labels for self-supervised signals. Stage 3, CML, takes unpaired pseudo pairs from the source and the target data as input, then employs a weight-sharing triple-branch transformer for cross-modal learning. During inference, only the self-attention transformer is required, without extra computational overhead.

<div align='center'>
  <img src="/images/research/CMESS-framework.png" width="500">  
</div>

## Visualization Results


1. Qualitative samples on the DDD17 dataset in the UDA setting. Compared to VID2E and ESS, our method is more accurate in predicting the details.
<div align='center'>
  <img src="/images/research/CMESS-fig1.png" width="500">  
</div>
2. Qualitative results on the DDD17 dataset in the supervised setting. The DDD17 annotations have limitations in terms of accuracy, but our method accurately distinguishes very small persons and objects.
<div align='center'>
  <img src="/images/research/CMESS-fig2.png" width="500">  
</div>
3. Qualitative samples on DSEC-Semantic in the UDA setting. Our method utilizes cross-attention to suppress the noise during alignment, leading to more precise segmentation results.
<div align='center'>
  <img src="/images/research/CMESS-fig3.png" width="500">  
</div>
4. Qualitative results on the DSEC-Semantic dataset in the supervised setting.
<div align='center'>
  <img src="/images/research/CMESS-fig4.png" width="500">  
</div>
5. The comparison of frame and event segmentation in overexposed scene (HDR).
<div align='center'>
  <img src="/images/research/CMESS-fig5.png" width="500">  
</div>
6. The heatmap of the cross-attention weights for a sample of pseudo pairs. As visualized in the figure, patches in the pseudo pairs with similar human semantic categories achieve higher weight distribution, which confirms the effectiveness of cross-attention.
<div align='center'>
  <img src="/images/research/CMESS-fig6.png" width="500">  
</div>

## Reference

Chuyun Xie, **Wei Gao\***, Ren Guo. Cross-modal Learning for Event-based Semantic Segmentation via Attention Soft Alignment. Submitted.